{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multilayer perceptron (MLP)  \n",
    "<p align=\"center\">\n",
    "  <img src=\"../files/input_layer.gif\" alt=\"MNIST 7\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPs are the \"hello world\" of neural networks. They are a type of artificial neural network that consist of multiple layers of interconnected **neurons**. They have an input layer, one or more **\"hidden layers\"**, and an output layer. \n",
    "\n",
    "You can think of a **neuron** as a function. Neurons take input values and deliver an output value called the **activation**; the inputs and output of a neuron are floating point numbers between 0 and 1. \n",
    "\n",
    "1. **Input layer**: The input layer receives the input values from the data. It has one neuron for each input feature.  \n",
    "  \n",
    "Imagine that we have images of hand-drawn digits between 0 and 9 that are 28 x 28 pixels. Each pixel has a floating point value betwee 0 and 1 representing the brightness of that pixel. If we \"flatten\" the image to produce a 1-dimensional vector, we would have a vector of length 784. We can think of this vector as the first layer of our network. Each pixel is a neuron and each grayscale value is that neuron's activation value.   \n",
    "  \n",
    "<p align=\"center\">\n",
    "  <img src=\"../files/pixels-as-input.png\" alt=\"MNIST 7\">\n",
    "</p>\n",
    "  \n",
    "An MLP is a **fully connected** network, which means that each neuron of a given layer is connected to every other neuron from the previous layer.\n",
    "\n",
    "2. **Hidden layers**: The hidden layers are those between the input layer and the final output layer and its where the \"magic\" happens. These layers process input values and produce intermediate output values (vectors of activation values). They are typically made up of multiple neurons. The number of hidden layers and the number of neurons in each layer are **hyperparameters** that can be tuned to optimize the performance of the network.  \n",
    "\n",
    "The connection between each neuron in a hidden layer in an MLP and the neurons in the layer preceeding it can be defined by the following function:  \n",
    "$$a_k^{(l)} = \\sigma(\\Sigma_{i=0}^n(w_{k,i} \\cdot a_i^{(l-1)}) + b_k$$\n",
    "Where:\n",
    "- a is the activation value of neuron k (or n) from layer l\n",
    "- w is the weight, which represents the strength of the connection between neurons k and n. \n",
    "- b is the bias parameter, which represents the constant term added to the weighted input. The bias parameter can be thought of as a \"threshold\" that controls how large or small the weighted sum of previous activations term $ \\left(\\sum_{i=0}^n w_{k,i} \\cdot a_i^{(l-1)}\\right) $ must be before it influences the activation value of neuron k in layer l. \n",
    "- σ is the activation function, which \"squeezes\" the output value of the neuron to a value between 0 and 1.\n",
    "  - The activation function is a non-linear transformation function that maps activation values that could range from $-\\infty$ to $\\infty$ to values that (classically) range from 0 to 1. Common activation functions include sigmoid, hyperbolic tangent, ReLU, and Leaky ReLU. (more on these later)\n",
    "\n",
    "The above can be simplified to represent the relationship between the activation values of all neurons in layer l to all neurons in layer l - 1.\n",
    "$$ a^{(l)} = \\sigma(W^{(l)} \\cdot a^{(l-1)}) + b^{(l)}) $$  \n",
    "  \n",
    "- a is a vector of all neuron activation values in layer l\n",
    "- W is a matrix of weights, where the rows represent the neurons in layer l and the columns represent the neurons in layer l - 1.\n",
    "- b is a vector of bias terms, where the elements represent the bias term for each neuron in layer l.\n",
    "\n",
    "Together, the weights and biases are the adjustable *parameters* of the neural network. Even in a simple fully connected network, the number of weights and biases can become quite large. For example, if we had input images of size 28 x 28, two hidden layers each with 5 neurons, and an output layer of 10 neurons for each digit 0 – 9, then the number of adjustable parameters would be:  \n",
    "|Layer | weights | biases|\n",
    "|------|--------|--------|\n",
    "|Input  | 784 * 5 | 5      |\n",
    "|Hidden 1| 5 * 5 | 5      |\n",
    "|Hidden 2| 5 * 5 | 5 |\n",
    "|Output | 5 * 10 | 10     |\n",
    "|Total | 4020 | 25 |\n",
    "\n",
    "\n",
    "**Summary:** every single neuron in layer l is connected to every single neuron in layer l - 1, and this connection involves a weight term specific to each neuron-neuron pair, a bias term specific to each neuron in layer l, and a pre-defined activation function to squeeze the output to be between 0 and 1.  \n",
    "\n",
    "3. **Output layer**: The output layer produces the final output values. In the case of a classification problem, the output layer typically has one neuron for each class, and the activation values of these neurons represent the \"probability\" of the input being in each class. The output layer uses a different activation function than the hidden layers, such as a softmax function ($ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} $)  \n",
    "\n",
    "## Activation functions\n",
    "Activation functions are mathematical functions that determine the output of a neuron given its input. There are many popular activation functions, such as:\n",
    "  \n",
    "- Sigmoid: $ ( S(x) = \\frac{1}{1 + e^{-x}} ) $\n",
    "- Hyperbolic tangent: $ ( \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} ) $\n",
    "- ReLU: $ ( \\max(0, x) ) $\n",
    "- Leaky ReLU: $ ( \\max(0.01x, x) ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training neural networks\n",
    "<p align=\"center\">\n",
    "  <img src=\"../files/mlp_training.gif\" alt=\"MNIST 7\">\n",
    "</p>\n",
    "Training neural networks involves adjusting the weights and biases of the neurons using a dataset of input-output pairs. The goal is to minimize the error between the predicted and actual output values. \n",
    "\n",
    "## Backpropagation\n",
    "Backpropagation is an algorithm used in training neural networks to minimize the error between the predicted and actual output values. It involves updating the weights and biases of the neurons in the network iteratively, using the chain rule to compute the gradient of the error function with respect to the weights and biases.\n",
    "\n",
    "Here’s how backpropagation works:\n",
    "\n",
    "### 1. **Forward Pass:**\n",
    "   - During the forward pass, the input data are passed through the network layer by layer. Each neuron performs a weighted sum of its inputs, applies an activation function, and passes the result to the next layer.\n",
    "\n",
    "### 2. **Loss Calculation:**\n",
    "   - Once the output is produced, a **loss function** (such as Mean Squared Error for regression or Cross-Entropy for classification) is used to measure the difference between the network's predicted output and the actual target.\n",
    "   - This loss represents the error in the network’s prediction.\n",
    "\n",
    "### 3. **Backward Pass (Backpropagation of Errors):**\n",
    "   - Backpropagation calculates how much each weight in the network contributed to the total error. This is done by applying the **chain rule of calculus** to propagate the error backward through the network, from the output layer to the input layer.\n",
    "   - The gradient of the loss with respect to each weight is computed. This gradient indicates how changing a weight will affect the loss.\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"left\" style=\"width: 50%;\">\n",
    "      <img src=\"../files/gradient_descent.gif\" alt=\"grad descent\">\n",
    "    </td>\n",
    "    <td align=\"right\" style=\"width: 50%;\">\n",
    "      <img src=\"../files/propagate-backwards.png\" alt=\"backprop\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "### 4. **Weight Update:**\n",
    "   - Once the gradients are calculated, the weights are updated using an optimization algorithm (typically **Stochastic Gradient Descent** or more advanced variants like **Adam**).\n",
    "   - The weights are adjusted in the direction that minimizes the error (i.e., in the opposite direction of the gradient), allowing the network to improve its predictions over time.\n",
    "\n",
    "### Key Concepts in Backpropagation:\n",
    "- **Chain Rule:** Backpropagation relies on the chain rule to efficiently compute gradients for each layer by propagating the errors backward, starting from the output.\n",
    "- **Gradient Descent:** The weights are updated based on the gradient of the loss with respect to each weight. The update rule is typically `w = w - η * ∂L/∂w`, where `η` is the learning rate and `∂L/∂w` is the gradient of the loss `L` with respect to weight `w`.\n",
    "- **Epochs:** The process of forward and backward passes is repeated for multiple epochs (iterations over the entire dataset) until the model converges to a solution with minimal error.\n",
    "\n",
    "There are several optimization algorithms and techniques used to train neural networks, such as:   \n",
    "- Gradient descent: Update the weights and biases using the formula: weight = weight - learning_rate * gradient\n",
    "- Stochastic gradient descent: Update the weights and biases using a single randomly chosen input-output pair\n",
    "## Overfitting and underfitting\n",
    "Overfitting occurs when a neural network learns the training data too well, capturing noise and not generalizing well to new, unseen data. Underfitting occurs when the neural network fails to capture the underlying patterns in the training data, resulting in poor performance on both the training and testing data.\n",
    "\n",
    "To prevent overfitting, you can use techniques such as:\n",
    "\n",
    "- Regularization: Add a penalty term to the loss function to encourage the weights and biases to be small and not too complex\n",
    "- Early stopping: Stop training the neural network when the validation loss starts to increase\n",
    "To prevent underfitting, you can try increasing the complexity of the neural network, using more hidden layers and neurons, or using techniques like dropout or data augmentation to artificially increase the size of the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code example\n",
    "## Explanation of Key Concepts\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "- The `MLP` class defines a simple Multi-Layer Perceptron with two hidden layers.\n",
    "- The input layer has 28*28 neurons (one for each pixel in the MNIST images).\n",
    "- The first hidden layer has 128 neurons, and the second hidden layer has 64 neurons.\n",
    "- The output layer has 10 neurons, corresponding to the 10 classes of digits (0-9).\n",
    "\n",
    "### Data Transformations and Augmentations\n",
    "\n",
    "- Data transformations are applied to normalize the images and convert them to tensors.\n",
    "- Data augmentation techniques like random rotations and translations are used to artificially increase the diversity of the training data, which can help improve the model's generalization.\n",
    "\n",
    "### Training Functions\n",
    "\n",
    "- The training loop iterates over the dataset multiple times (epochs).\n",
    "- For each batch of images, the model performs a forward pass to compute the outputs.\n",
    "- The loss is computed using the `CrossEntropyLoss` function, which measures the difference between the predicted and true labels.\n",
    "- Backpropagation is performed to compute the gradients of the loss with respect to the model's parameters.\n",
    "- The optimizer updates the model's parameters using the computed gradients.\n",
    "\n",
    "### Backpropagation and Loss\n",
    "\n",
    "- Backpropagation is the process of computing the gradients of the loss function with respect to the model's parameters.\n",
    "- The loss function used here is `CrossEntropyLoss`, which is commonly used for classification tasks.\n",
    "- The optimizer (Adam) updates the model's parameters to minimize the loss.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "- The model is evaluated on the test dataset to measure its accuracy.\n",
    "- The accuracy is computed as the percentage of correctly classified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define the layers of the neural network\n",
    "        self.fc1 = nn.Linear(28*28, 128)  # First fully connected layer (input to hidden)\n",
    "        self.fc2 = nn.Linear(128, 64)     # Second fully connected layer (hidden to hidden)\n",
    "        self.fc3 = nn.Linear(64, 10)      # Third fully connected layer (hidden to output)\n",
    "        self.relu = nn.ReLU()             # ReLU activation function\n",
    "        self.prelu = nn.PReLU() # PreLU activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)  # LogSoftmax activation function for output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = x.view(-1, 28*28)  # Flatten the input image\n",
    "        x = self.relu(self.fc1(x))  # Apply ReLU after first layer\n",
    "        x = self.relu(self.fc2(x))  # Apply ReLU after second layer\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return self.softmax(x)  # Apply LogSoftmax to the output\n",
    "\n",
    "# Load the MNIST dataset\n",
    "use_augmentation = True\n",
    "if not use_augmentation:\n",
    "    # without data augmentation\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "else:\n",
    "    # with data augmentation\n",
    "    # Define the data augmentation and normalization transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),  # Randomly rotate the image by 10 degrees\n",
    "        transforms.RandomAffine(0, translate=(0.1, 0.1)),  # Randomly translate the image\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)) # Normalize the image\n",
    "    ])\n",
    "\n",
    "    # Load the MNIST dataset with the defined transforms\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ]))\n",
    "\n",
    "# Use a small subset of the dataset for training\n",
    "subset_indices = list(range(1000))  # Use the first 1000 samples\n",
    "mnist_train_subset = Subset(mnist_train, subset_indices)\n",
    "\n",
    "# Visualize one of the training images\n",
    "image, label = mnist_train_subset[0]\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f'Label: {label}')\n",
    "plt.show()\n",
    "\n",
    "# Define the dataloaders for training and testing datasets\n",
    "train_loader = DataLoader(mnist_train_subset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()  # Using Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Using Adam optimizer\n",
    "\n",
    "# Training loop with visualization\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass (compute gradients)\n",
    "        optimizer.step()  # Update the weights\n",
    "        running_loss += loss.item()  # Accumulate the loss\n",
    "    \n",
    "    train_losses.append(running_loss / len(train_loader))  # Average loss for the epoch\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)  # Forward pass\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the predicted class\n",
    "            total += labels.size(0)  # Total number of samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "    \n",
    "    test_accuracies.append(100 * correct / total)  # Accuracy for the epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "# Plotting the training loss and test accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), test_accuracies, marker='o')\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
